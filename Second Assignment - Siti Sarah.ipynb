{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siti Sarah Azli Zuhairey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### From the exercises in Data Camp, demonstrate on how you can use kNN, Decision Tree and Naive Bayes to perform classification and regression, on Jupyter Notebook.\n",
    "\n",
    "#### kNN\n",
    "\n",
    "#Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Create arrays for the features and the response variable\n",
    "y = df['party'].values\n",
    "X = df.drop('party', axis=1).values\n",
    "#Create a k-NN classifier with 6 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2., 2.]])\n",
    "array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n",
    "Number of mislabeled points out of a total 150 points : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### 10 papers that have applied kNN, Decision Tree and Naive Bayes to solve their problems.\n",
    "\n",
    "#### 1\n",
    "**Prediction of Heart Disease Based on Decision Trees**\n",
    "\n",
    "Decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one of data mining technique that is easy to implement and where huge data can be applied onto. Thus, decision tree algorithm has been widely used in clinical decision support system. Decision trees is now used in heart disease prediction model. The prediction of heart disease is most entangled assignment in the field of medical sciences which cannot be observed with a naked eye and comes instantly anywhere, anytime. So there arises a need to build up a decision support system for detecting heart disease. The prediction model is based on patient’s age, gender, chest pain, the resting blood pressure in mmHg, serum cholesterol in mg/d, hereditary, fasting blood pressure in mg/dl, thal and smoking.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Lakshmishree, J., & Paramesha, K. (2017, May). Prediction of Heart Disease Based on Decision Trees. International Journal for Research in Applied Science & Engineering Technology (IJRASET) (Volume 5 Issue V)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2\n",
    "**Application of String Vector based K Nearest Neighbor to Content based Segmentation of each News Article**\n",
    "\n",
    "The text segmentation is alluded to the way toward sectioning a long content into subtexts dependent on its substance. The research using string vector based KNN as the approach to the text segmentation. KNN works when the sample paragraph pairs which are labeled with boundary are gathered domain by domain, and they are encoded into string vectors. By sliding paragraphs, a list of adjacent paragraph pairs is generated from the text which is assumed to be tagged with its own domain, and their similarities with sample ones in the corresponding domain are computed. For each paragraph pair, its k nearest sample ones are selected and its label is decided by voting their labels.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Taeho, J. (2019). Application of String Vector based K Nearest Neighbor to Content based Segmentation of each News Article. School of Game Hongik University Sejong, South Korea (Vol. 2 No. 1). ICAEIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3\n",
    "**Private Naïve Bayes Classification of Personal Biomedical Data: Application in Cancer Data Analysis**\n",
    "\n",
    "The notion of privacy in the healthcare domain is at least as old as the ancient Greeks. Thus, biomedical researchers and clinicians found it difficult for them to do further analysis. Naïve Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. Naive Bayes is known to outperform even highly sophisticated classification methods. Private-key fully homomorphic encryption used to design a cryptographic protocol for private Naive Bayes classification. This protocol allows a data owner to privately classify his or her information without direct access to the learned model. This algorithm is applied in cancer data analysis which it can classify medical data efﬁciently.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Wood, A., Shpilrain, V., Najarian, K., & Kahrobaei, D. (2019). Private naive bayes classification of personal biomedical data: Application in cancer data analysis. Computers in biology and medicine, 105, 144-150.\n",
    "\n",
    "#### 4\n",
    "**The Application of Improved Decision Tree Algorithm in Data Mining of Employment Rate: Evidence from China**\n",
    "\n",
    "Improvements to ID3 Decision Tree Classifier algorithm has been proposed in this paper that is used in the data mining of employment rate in China. The data set that was used in testing the algorithm was for school students and their employment rate. The proposed enhancements conquered two inadequacies of ID3, which are taking data picked up as its arrangement assessment capacity to choose ideal characteristic. The proposed enhancements can be closed in two primary concerns. To start with, including characteristic measure which will be contrasted and the past qualities, and the higher esteem the measure esteem, the more data will be given to classifier. Second, presenting the rate of data picked up by isolating data gain by data separated/split, and that tackled the ID3 multi-esteem issue.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Shao, Y., Chen, Q., & Yin, W. (2009, April). The Application of Improved Decision Tree Algorithm in Data Mining of Employment Rate: Evidence from China. In 2009 First International Workshop on Database Technology and Applications (pp. 202-205). IEEE.\n",
    "\n",
    "#### 5\n",
    "**A New Approach for Off-Line Handwritten Arabic Word Recognition Using KNN Classifier**\n",
    "\n",
    "Arabic Handwriting is viewed as hard to perceive as it is like some different dialects. The paper examined the use of KNN Classifier for Arabic Handwriting Recognition offline system. Thus, KNN Classifier was utilized for order and effectively has been tried on IFN/ENIT database. The system examined three distinct stages including preprocessing , where content is standardized and separated, highlight extraction to decrease excess, and arrangement utilizing KNN multi-class characterization. The recognition rate was influenced by the nature of information pictures of content, and the viability and pre-preparing. The utilized dataset was taken from IFN/ENIT database, which contains the names of Tunisian towns/towns postcodes composed by in excess of 1000 individuals who were approached to compose their names just as filling in a few structures identified with their towns/towns postcodes. To entirety up, KNN was utilized to order unlabeled testing set and demonstrated to beat a few existing strategies in Arabic Handwritten Arabic word recognition.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "AlKhateeb, J. H., Khelifi, F., Jiang, J., & Ipson, S. S. (2009, November). A new approach for off-line handwritten Arabic word recognition using KNN classifier. In 2009 IEEE International Conference on Signal and Image Processing Applications (pp. 191-194). IEEE.\n",
    "\n",
    "#### 6\n",
    "**Application of the Naive Bayes Method to a Decision Support System to provide Discounts**\n",
    "\n",
    "This paper talked about the use of Naive Bayes Classifier in allowing discounts utilizing decision support system. The specialists were endeavoring to illuminate the issue of giving discounts for multi-criteria systems. The criteria was recognized by PT. Bina Usaha Teknik as it was the contextual analysis for the application just as the wellspring of dataset. The proposed system was utilized to disentangle huge measure of data to help right decision making. The utilization of Naive Bayes Classifier helped in conceding progressively proficient and powerful discounts for organization representatives. The system was created with RAD ( Rapid Application looking for Development) utilizing UML (Unified Modeling Language). It tends to be said that the system prevailing to give elective in figuring value discounts proposals dependent on the last likelihood by any blend of factors/criteria.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "F. Burdi, A. H. Setianingrum and N. Hakiem, \"Application of the Naive Bayes Method to a Decision Support System to Provide Discounts (Case Study: PT. Bina Usaha Teknik),\" 2016 6th International Conference on Information and Communication Technology for The Muslim World (ICT4M), Jakarta, 2016, pp. 281-285.\n",
    "\n",
    "#### 7\n",
    "**MFZ-KNN-A Modified Fuzzy Based K Nearest Neighbor Algorithm**\n",
    "\n",
    "This paper proposed an improved fuzzy based KNN calculation MFZ-KNN utilizing the clustering strategy. In this calculation, the fuzzy groups are created to decide the fuzzy territories in the informational collection preceding arrangement utilizing FCM calculation. After that the test information is arranged utilizing enrollment network (yield of FCM calculation) and KNN calculation. fuzzy bunches are gotten at preprocessing step and the participation of the preparation informational collection is figured in reference with the centroid of the groups. This decreases the intricacy of time astoundingly. The outcomes demonstrate that it is superior to both customary KNN and fuzzy KNN regarding precision and time.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Taneja, S., Gupta, C., Aggarwal, S., & Jindal, V. (2015, March). MFZ-KNN—a modified fuzzy based K nearest neighbor algorithm. In 2015 International Conference on Cognitive Computing and Information Processing (CCIP) (pp. 1-5). IEEE.\n",
    "\n",
    "#### 8\n",
    "**Inference-Based Naive Bayes: Turning Naive Bayes Cost-Sensitive**\n",
    "\n",
    "An essential test for building up a cost-sensitive Naive Bayes strategy is the means by which to successfully classify an occasion dependent on the cost-sensitive edge registered under the supposition of knowing the example's actual characterization probabilities and the exceedingly one-sided estimations of these probabilities by the Naive Bayes technique. To address this test, this paper presented a cost-sensitive Naive Bayes strategy from a novel point of view of surmising the request connection (e.g., more prominent than or equivalent to, not exactly) between a case's actual characterization likelihood of having a place with the class of intrigue and the cost-sensitive edge. this technique learns and deduces the request connection from the preparation information and characterizes the occurrence dependent on the induced request connection. UCI informational indexes and a certifiable contextual analysis.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Fang, X. (2013). Inference-based naive bayes: Turning naive bayes cost-sensitive. IEEE Transactions on Knowledge and Data Engineering, 25(10), 2302-2313.\n",
    "\n",
    "#### 9\n",
    "**Eager Decision Tree**\n",
    "\n",
    "Decision Tree acceptance is regularly utilized classification algorithm. One of the vital issues is the way to utilize records with obscure qualities from preparing just as testing information. Numerous methodologies have been proposed to address the effect of obscure qualities at preparing on precision of prediction. Nonetheless, not very many strategies are there to address the issue in testing data.In Lazy Decision Tree, the issue of obscure quality qualities in test case is totally wiped out by postponing the development of tree till the classification time and utilizing just known traits for classification. This paper present novel algorithm 'Eager Decision Tree' which develops a solitary prediction display at the season of preparing which considers all conceivable outcomes of obscure trait esteems from testing information. It normally evacuates the issue of giving obscure qualities in testing information in Decision Tree acceptance like Lazy Decision Tree.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Gavankar, S. S., & Sawarkar, S. D. (2017, April). Eager decision tree. In 2017 2nd International Conference for Convergence in Technology (I2CT) (pp. 837-840). IEEE.\n",
    "\n",
    "#### 10\n",
    "**Forecasting Copper Prices by Decision Tree Learning**\n",
    "\n",
    "In this paper, a machine-learning algorithm based on decision tree has been executed to foresee the copper prices. The outcome is fairly fulfilling by ready to foresee the future copper prices precisely with a blunder underneath 5%. For information preparation, prices of all others items, for example, gold, silver, unrefined petroleum and so on have been gathered, and the correlation among copper and every single other product have been investigated to decide the highlights for preparing. From the correlation examines, the prices of raw petroleum, gold , silver and different wares are profoundly related with copper. Henceforth, those factors are utilized as info information in a type of 1x8 cluster. There are two essential parameter, G and D utilized in this decision tree display, where D = cost of other related factors on past D days utilized AS INPUT, and G is the G-th day after tomorrow for the prediction. The model has been tried by utilizing distinctive D and G parameters, and the outcomes were great where the RMSE are < 5% all things considered.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Liu, C., Hu, Z., Li, Y., & Liu, S. (2017). Forecasting copper prices by decision tree learning. Resources Policy, 52, 427-434.\n",
    "\n",
    "#### 11\n",
    "**A Feature Dependent Naive Bayes Approach And Its Application To The Software Defect Prediction Problem**\n",
    "\n",
    "A Feature Dependent Naive Bayes (FDNB) classification has been proposed to software defect prediction problem by utilizing NASA PROMISE datasets. The measurements that have been utilized are Line of Code ( LoC ), McCabe, Halstead and others. Feature choice have been actualized to sift through measurements that are not noteworthy for prediction of software defect. Connection based determination channel strategy was utilized and a heuristic inquiry calculation has been executed to extricate the best feature subsets. The preprocessing of the information was proceeded by doing standardization, discretization to improve the classifier execution. The discretization is utilized to part the numerical estimations of different software metric into straight out qualities, and utilized for likelihood dispersion figuring in the preparation and testing.The last advance of the preprocessing is the class appropriation balancer. The appropriation rebalancing is required because of the way that non-defective modules dwarfs defective modules in an expansive degree which will prompt examining issues. To additionally improve the execution of the classifier, FDNB has been actualized to guarantee the feature freedom are genuinely accomplished. It was finished by considering likelihood of event of each feature alongside others. The execution of FDNB classifier was contrasted with different sorts of Naive Bayes classifier and the aftereffect of FDNB is somewhat better where less false caution revealed.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Arar, Ö. F., & Ayan, K. (2017). A feature dependent Naive Bayes approach and its application to the software defect prediction problem. Applied Soft Computing, 59, 197-209.\n",
    "\n",
    "#### 12\n",
    "**A Naive SVM-KNN Based Stock Market Trend Reversal Analysis for Indian Benchmark Indices**\n",
    "\n",
    "Hybridized framework of Support Vector Machine ( SVM ) with K-Nearest Neighbor approach has been proposed to perform Indian stock market lists forecast. The objective of the examination is to foresee the end value, unpredictability and force of stock market dependent on accessible information. The dataset utilized in this investigation are BSE SENSEX and CNX Nifty, where those two are stock trade in Asia. The initial step of the preprocessing will be time arrangement dataset age, which comprises of stock opening, stock lower, stock higher and stock shutting cost for quickly. Next, different stock specialized pointers, for example, RSI, ATR, Williams%R, MA were incorporated as a major aspect of the info information. The model works in three stages, in stage 1, the dataset was refreshed utilizing the preprocessing strategy referenced before. Next in stage 2, datasets is partitioned into two sections and preparing was finished utilizing SVM classifier. In stage 3, information is arranged utilizing SVM and K nearest neighbors is looked to recognize the separation. The execution of half and half KNN + SVM show has been contrasted with FLIT2FNS and CEFLANN. The execution of half and half KNN + SVM show is superior to demonstrated that it is valuable not exclusively to anticipate shutting cost, yet additionally ready to distinguish volatitlitty and stock energy.\n",
    "\n",
    "*Reference:*\n",
    "\n",
    "Nayak, R. K., Mishra, D., & Rath, A. K. (2015). A Naïve SVM-KNN based stock market trend reversal analysis for Indian benchmark indices. Applied Soft Computing, 35, 670-680."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
